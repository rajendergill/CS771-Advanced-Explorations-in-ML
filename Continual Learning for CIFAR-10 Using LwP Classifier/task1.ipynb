{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Task 1"
      ],
      "metadata": {
        "id": "T4APgFAxQUJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpiJDjPhQFJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f6d418-904f-44af-bd4c-705bb876f497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Necessary Libraries Imported\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "print(\"All Necessary Libraries Imported\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "o7UKgjB0Qbea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ConvNeXtBase model as a feature extractor\n",
        "convnext_base = models.convnext_base(weights=models.ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
        "convnext_base = convnext_base.to(device)\n",
        "convnext_base.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_QTMyFOQh2-",
        "outputId": "2dad411b-0983-4726-bf7a-65458c16db56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNeXt(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
              "      (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.014285714285714285, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.02857142857142857, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.04285714285714286, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.07142857142857142, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.08571428571428572, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
              "      )\n",
              "      (3): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.12857142857142856, mode=row)\n",
              "      )\n",
              "      (4): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.14285714285714285, mode=row)\n",
              "      )\n",
              "      (5): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.15714285714285714, mode=row)\n",
              "      )\n",
              "      (6): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
              "      )\n",
              "      (7): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.18571428571428572, mode=row)\n",
              "      )\n",
              "      (8): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
              "      )\n",
              "      (9): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.21428571428571427, mode=row)\n",
              "      )\n",
              "      (10): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
              "      )\n",
              "      (11): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.24285714285714285, mode=row)\n",
              "      )\n",
              "      (12): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2571428571428571, mode=row)\n",
              "      )\n",
              "      (13): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2714285714285714, mode=row)\n",
              "      )\n",
              "      (14): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
              "      )\n",
              "      (15): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
              "      )\n",
              "      (16): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.3142857142857143, mode=row)\n",
              "      )\n",
              "      (17): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.32857142857142857, mode=row)\n",
              "      )\n",
              "      (18): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
              "      )\n",
              "      (19): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.35714285714285715, mode=row)\n",
              "      )\n",
              "      (20): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.37142857142857144, mode=row)\n",
              "      )\n",
              "      (21): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.38571428571428573, mode=row)\n",
              "      )\n",
              "      (22): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
              "      )\n",
              "      (23): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4142857142857143, mode=row)\n",
              "      )\n",
              "      (24): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.42857142857142855, mode=row)\n",
              "      )\n",
              "      (25): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.44285714285714284, mode=row)\n",
              "      )\n",
              "      (26): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.45714285714285713, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4714285714285714, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4857142857142857, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "RzwO2FhOQx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.images = data['data']\n",
        "        self.labels = torch.tensor(data['targets']) if 'targets' in data else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.fromarray(self.images[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx] if self.labels is not None else -1\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "WTl3HtniQzh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction function\n",
        "def extract_features(data_loader, model):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels_batch in data_loader:\n",
        "            images = images.to(device)\n",
        "            batch_features = model(images)\n",
        "            batch_features = batch_features.view(batch_features.size(0), -1)  # Flatten\n",
        "            features.append(batch_features.cpu())\n",
        "            labels.append(labels_batch)\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "# Train Learning with Prototypes model\n",
        "def train_lwp(features, labels):\n",
        "    unique_classes = torch.unique(labels)\n",
        "    class_means = {}\n",
        "    for cls in unique_classes:\n",
        "        class_indices = (labels == cls)\n",
        "        class_means[cls.item()] = features[class_indices].mean(dim=0)\n",
        "    return class_means\n",
        "\n",
        "# Predict using Learning with Prototypes (Batch Processing)\n",
        "def predict_lwp(features, class_means):\n",
        "    # Convert class means to a tensor for batch processing\n",
        "    mean_tensor = torch.stack(list(class_means.values()))\n",
        "    mean_classes = torch.tensor(list(class_means.keys()))\n",
        "\n",
        "    # Calculate distances between features and class means\n",
        "    distances = torch.cdist(features, mean_tensor)\n",
        "    closest_indices = torch.argmin(distances, dim=1)\n",
        "    return mean_classes[closest_indices]"
      ],
      "metadata": {
        "id": "xH4jqmjnRDZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "def task_1(data_paths, heldout_paths):\n",
        "    print(\"Starting Task 1...\")\n",
        "    models_task1 = []\n",
        "    accuracy_matrix = []\n",
        "\n",
        "    # Precompute held-out dataset features to avoid recomputation\n",
        "    heldout_features_cache = {}\n",
        "    print(\"Precomputing features for datasets...\")\n",
        "    for i, path in enumerate(heldout_paths):\n",
        "        print(f\"Loading and processing dataset D{i+1}...\")\n",
        "        heldout_data = torch.load(path)\n",
        "        heldout_dataset = CustomImageDataset(heldout_data, transform=transform)\n",
        "        heldout_loader = DataLoader(heldout_dataset, batch_size=32, num_workers=4, shuffle=False)\n",
        "        features, labels = extract_features(heldout_loader, convnext_base)\n",
        "        heldout_features_cache[i] = (features, labels)\n",
        "    print(\"Held-out dataset preprocessing completed.\")\n",
        "\n",
        "    # Iterate through datasets\n",
        "    for i in range(10):\n",
        "        print(f\"\\nProcessing Dataset D{i+1}...\")\n",
        "\n",
        "        # Load datasets\n",
        "        train_data = torch.load(data_paths[i])\n",
        "        train_dataset = CustomImageDataset(train_data, transform=transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=False)\n",
        "\n",
        "        if i == 0:\n",
        "            print(\"Training initial model f1 on D1...\")\n",
        "            train_features, train_labels = extract_features(train_loader, convnext_base)\n",
        "            class_means = train_lwp(train_features, train_labels)\n",
        "            models_task1.append(class_means)\n",
        "            print(\"Initial model f1 training completed.\")\n",
        "        else:\n",
        "            print(f\"Predicting pseudo-labels for Dataset D{i+1} using model f{i}...\")\n",
        "            unlabeled_data = torch.load(data_paths[i])\n",
        "            unlabeled_dataset = CustomImageDataset(unlabeled_data, transform=transform)\n",
        "            unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, num_workers=4, shuffle=False)\n",
        "\n",
        "            unlabeled_features, _ = extract_features(unlabeled_loader, convnext_base)\n",
        "            pseudo_labels = predict_lwp(unlabeled_features, models_task1[-1])\n",
        "\n",
        "            # Update model with pseudo-labels\n",
        "            print(\"Updating model with pseudo-labeled data...\")\n",
        "            class_means_update = train_lwp(unlabeled_features, pseudo_labels)\n",
        "            for cls in class_means_update:\n",
        "                if cls in models_task1[-1]:\n",
        "                    models_task1[-1][cls] = (models_task1[-1][cls] + class_means_update[cls]) / 2\n",
        "                else:\n",
        "                    models_task1[-1][cls] = class_means_update[cls]\n",
        "            models_task1.append(models_task1[-1])\n",
        "            print(f\"Model f{i+1} updated successfully.\")\n",
        "\n",
        "        # Evaluate on held-out datasets\n",
        "        print(f\"Evaluating model f{i+1} on datasets...\")\n",
        "        row_accuracies = []\n",
        "        for j in range(i + 1):\n",
        "            heldout_features, heldout_labels = heldout_features_cache[j]\n",
        "            predictions = predict_lwp(heldout_features, models_task1[-1])\n",
        "            accuracy = (predictions == heldout_labels).float().mean().item() * 100\n",
        "            row_accuracies.append(accuracy)\n",
        "            print(f\"Evaluation on D̂{j+1}: Accuracy = {accuracy:.2f}%\")\n",
        "        accuracy_matrix.append(row_accuracies)\n",
        "        print(f\"Model f{i+1} evaluation completed. Current accuracy matrix row: {row_accuracies}\")\n",
        "\n",
        "    # Print accuracy matrix\n",
        "    print(\"\\nTask 1 completed. Accuracy Matrix:\")\n",
        "    print(\"     \" + \"  \".join([f\"D̂{i+1}\" for i in range(10)]))\n",
        "    for i, row in enumerate(accuracy_matrix):\n",
        "        print(f\"f{i+1}: \" + \"  \".join([f\"{acc:.2f}%\" for acc in row]))\n",
        "\n",
        "    print(\"\\nAll tasks completed successfully!\")\n",
        "    return models_task1, accuracy_matrix"
      ],
      "metadata": {
        "id": "BP033k2OTD3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update these paths to point to the actual dataset locations\n",
        "data_paths = [f\"/content/drive/MyDrive/dataset/part_one_dataset/train_data/{i+1}_train_data.tar.pth\" for i in range(10)]\n",
        "heldout_paths = [f\"/content/drive/MyDrive/dataset/part_one_dataset/eval_data/{i+1}_eval_data.tar.pth\" for i in range(10)]\n",
        "\n",
        "# Run Task 1\n",
        "models_task1, accuracy_matrix = task_1(data_paths, heldout_paths)\n",
        "\n",
        "torch.save(models_task1, \"/content/drive/MyDrive/models_task1.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBdiSFSHTLCq",
        "outputId": "522219ae-0f4f-4edb-fb96-91d8886ea1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Task 1...\n",
            "Precomputing features for datasets...\n",
            "Loading and processing dataset D1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-04e19feee6c5>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  heldout_data = torch.load(path)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and processing dataset D2...\n",
            "Loading and processing dataset D3...\n",
            "Loading and processing dataset D4...\n",
            "Loading and processing dataset D5...\n",
            "Loading and processing dataset D6...\n",
            "Loading and processing dataset D7...\n",
            "Loading and processing dataset D8...\n",
            "Loading and processing dataset D9...\n",
            "Loading and processing dataset D10...\n",
            "Held-out dataset preprocessing completed.\n",
            "\n",
            "Processing Dataset D1...\n",
            "Training initial model f1 on D1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-04e19feee6c5>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_data = torch.load(data_paths[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model f1 training completed.\n",
            "Evaluating model f1 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 89.00%\n",
            "Model f1 evaluation completed. Current accuracy matrix row: [88.99999856948853]\n",
            "\n",
            "Processing Dataset D2...\n",
            "Predicting pseudo-labels for Dataset D2 using model f1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-04e19feee6c5>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  unlabeled_data = torch.load(data_paths[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating model with pseudo-labeled data...\n",
            "Model f2 updated successfully.\n",
            "Evaluating model f2 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 88.44%\n",
            "Evaluation on D̂2: Accuracy = 89.76%\n",
            "Model f2 evaluation completed. Current accuracy matrix row: [88.44000101089478, 89.7599995136261]\n",
            "\n",
            "Processing Dataset D3...\n",
            "Predicting pseudo-labels for Dataset D3 using model f2...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f3 updated successfully.\n",
            "Evaluating model f3 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 87.84%\n",
            "Evaluation on D̂2: Accuracy = 89.08%\n",
            "Evaluation on D̂3: Accuracy = 87.48%\n",
            "Model f3 evaluation completed. Current accuracy matrix row: [87.84000277519226, 89.07999992370605, 87.48000264167786]\n",
            "\n",
            "Processing Dataset D4...\n",
            "Predicting pseudo-labels for Dataset D4 using model f3...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f4 updated successfully.\n",
            "Evaluating model f4 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 87.72%\n",
            "Evaluation on D̂2: Accuracy = 89.08%\n",
            "Evaluation on D̂3: Accuracy = 87.40%\n",
            "Evaluation on D̂4: Accuracy = 88.68%\n",
            "Model f4 evaluation completed. Current accuracy matrix row: [87.72000074386597, 89.07999992370605, 87.40000128746033, 88.67999911308289]\n",
            "\n",
            "Processing Dataset D5...\n",
            "Predicting pseudo-labels for Dataset D5 using model f4...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f5 updated successfully.\n",
            "Evaluating model f5 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 87.32%\n",
            "Evaluation on D̂2: Accuracy = 89.24%\n",
            "Evaluation on D̂3: Accuracy = 87.32%\n",
            "Evaluation on D̂4: Accuracy = 88.52%\n",
            "Evaluation on D̂5: Accuracy = 87.68%\n",
            "Model f5 evaluation completed. Current accuracy matrix row: [87.3199999332428, 89.24000263214111, 87.3199999332428, 88.5200023651123, 87.6800000667572]\n",
            "\n",
            "Processing Dataset D6...\n",
            "Predicting pseudo-labels for Dataset D6 using model f5...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f6 updated successfully.\n",
            "Evaluating model f6 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 87.20%\n",
            "Evaluation on D̂2: Accuracy = 88.68%\n",
            "Evaluation on D̂3: Accuracy = 86.80%\n",
            "Evaluation on D̂4: Accuracy = 88.40%\n",
            "Evaluation on D̂5: Accuracy = 87.44%\n",
            "Evaluation on D̂6: Accuracy = 87.20%\n",
            "Model f6 evaluation completed. Current accuracy matrix row: [87.1999979019165, 88.67999911308289, 86.79999709129333, 88.40000033378601, 87.44000196456909, 87.1999979019165]\n",
            "\n",
            "Processing Dataset D7...\n",
            "Predicting pseudo-labels for Dataset D7 using model f6...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f7 updated successfully.\n",
            "Evaluating model f7 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 87.12%\n",
            "Evaluation on D̂2: Accuracy = 88.68%\n",
            "Evaluation on D̂3: Accuracy = 86.88%\n",
            "Evaluation on D̂4: Accuracy = 88.40%\n",
            "Evaluation on D̂5: Accuracy = 87.16%\n",
            "Evaluation on D̂6: Accuracy = 87.24%\n",
            "Evaluation on D̂7: Accuracy = 87.80%\n",
            "Model f7 evaluation completed. Current accuracy matrix row: [87.12000250816345, 88.67999911308289, 86.87999844551086, 88.40000033378601, 87.15999722480774, 87.23999857902527, 87.8000020980835]\n",
            "\n",
            "Processing Dataset D8...\n",
            "Predicting pseudo-labels for Dataset D8 using model f7...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f8 updated successfully.\n",
            "Evaluating model f8 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 86.84%\n",
            "Evaluation on D̂2: Accuracy = 88.40%\n",
            "Evaluation on D̂3: Accuracy = 86.48%\n",
            "Evaluation on D̂4: Accuracy = 88.28%\n",
            "Evaluation on D̂5: Accuracy = 87.28%\n",
            "Evaluation on D̂6: Accuracy = 87.20%\n",
            "Evaluation on D̂7: Accuracy = 87.48%\n",
            "Evaluation on D̂8: Accuracy = 87.64%\n",
            "Model f8 evaluation completed. Current accuracy matrix row: [86.8399977684021, 88.40000033378601, 86.4799976348877, 88.27999830245972, 87.27999925613403, 87.1999979019165, 87.48000264167786, 87.63999938964844]\n",
            "\n",
            "Processing Dataset D9...\n",
            "Predicting pseudo-labels for Dataset D9 using model f8...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f9 updated successfully.\n",
            "Evaluating model f9 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 86.88%\n",
            "Evaluation on D̂2: Accuracy = 88.44%\n",
            "Evaluation on D̂3: Accuracy = 86.96%\n",
            "Evaluation on D̂4: Accuracy = 88.00%\n",
            "Evaluation on D̂5: Accuracy = 86.88%\n",
            "Evaluation on D̂6: Accuracy = 87.12%\n",
            "Evaluation on D̂7: Accuracy = 87.32%\n",
            "Evaluation on D̂8: Accuracy = 87.44%\n",
            "Evaluation on D̂9: Accuracy = 87.56%\n",
            "Model f9 evaluation completed. Current accuracy matrix row: [86.87999844551086, 88.44000101089478, 86.9599997997284, 87.99999952316284, 86.87999844551086, 87.12000250816345, 87.3199999332428, 87.44000196456909, 87.55999803543091]\n",
            "\n",
            "Processing Dataset D10...\n",
            "Predicting pseudo-labels for Dataset D10 using model f9...\n",
            "Updating model with pseudo-labeled data...\n",
            "Model f10 updated successfully.\n",
            "Evaluating model f10 on datasets...\n",
            "Evaluation on D̂1: Accuracy = 86.84%\n",
            "Evaluation on D̂2: Accuracy = 88.48%\n",
            "Evaluation on D̂3: Accuracy = 86.92%\n",
            "Evaluation on D̂4: Accuracy = 88.24%\n",
            "Evaluation on D̂5: Accuracy = 87.04%\n",
            "Evaluation on D̂6: Accuracy = 87.36%\n",
            "Evaluation on D̂7: Accuracy = 87.32%\n",
            "Evaluation on D̂8: Accuracy = 87.48%\n",
            "Evaluation on D̂9: Accuracy = 87.60%\n",
            "Evaluation on D̂10: Accuracy = 87.52%\n",
            "Model f10 evaluation completed. Current accuracy matrix row: [86.8399977684021, 88.48000168800354, 86.91999912261963, 88.23999762535095, 87.04000115394592, 87.36000061035156, 87.3199999332428, 87.48000264167786, 87.59999871253967, 87.51999735832214]\n",
            "\n",
            "Task 1 completed. Accuracy Matrix:\n",
            "     D̂1  D̂2  D̂3  D̂4  D̂5  D̂6  D̂7  D̂8  D̂9  D̂10\n",
            "f1: 89.00%\n",
            "f2: 88.44%  89.76%\n",
            "f3: 87.84%  89.08%  87.48%\n",
            "f4: 87.72%  89.08%  87.40%  88.68%\n",
            "f5: 87.32%  89.24%  87.32%  88.52%  87.68%\n",
            "f6: 87.20%  88.68%  86.80%  88.40%  87.44%  87.20%\n",
            "f7: 87.12%  88.68%  86.88%  88.40%  87.16%  87.24%  87.80%\n",
            "f8: 86.84%  88.40%  86.48%  88.28%  87.28%  87.20%  87.48%  87.64%\n",
            "f9: 86.88%  88.44%  86.96%  88.00%  86.88%  87.12%  87.32%  87.44%  87.56%\n",
            "f10: 86.84%  88.48%  86.92%  88.24%  87.04%  87.36%  87.32%  87.48%  87.60%  87.52%\n",
            "\n",
            "All tasks completed successfully!\n"
          ]
        }
      ]
    }
  ]
}